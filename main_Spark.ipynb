{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Spark\n",
    "with open(\"setupPySpark.py\", \"r\") as setup_file:\n",
    "    exec(setup_file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark context\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL context\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required functions\n",
    "from util import read_file, read_folder, get_character_similarity\n",
    "from pandas import Series, DataFrame\n",
    "from util_spark import remove_stopwords_spark, detect_language_spark, flatten_list_of_tokens, spell_correct_tokens_spark, get_semantic_similarity_spark\n",
    "from tokenization_spark import tokenize_sentence_nltk_spark\n",
    "from pyspark.sql.functions import col\n",
    "from pos_tagging_spark import run_treetagger_pos_tag_spark\n",
    "from modeling_spark import run_word2vec_model_pyspark\n",
    "from json import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input file(s) using python's default libraries\n",
    "in_file = load(open(\"in_file.cfg\"))\n",
    "patterns_file = in_file[\"patterns_file\"]\n",
    "file_folder = in_file[\"file_folder\"]\n",
    "label = in_file[\"label\"]\n",
    "column = in_file[\"column\"]\n",
    "in_type = in_file[\"in_type\"]\n",
    "in_file = in_file[\"in_file\"]\n",
    "if file_folder == \"file\":\n",
    "    strings = read_file(in_file, in_type = in_type)\n",
    "    if in_type == \"text\":\n",
    "        strings = tokenize_sentence_nltk(strings)\n",
    "        strings = DataFrame(strings)[0]\n",
    "    elif in_type == \"html_chat\":\n",
    "        timestamp = strings[2]\n",
    "        meta_data = strings[1]\n",
    "        strings = strings[0]\n",
    "        strings[label] = meta_data[\"Comment\"]\n",
    "        labels = strings[label]\n",
    "        strings = strings[column]\n",
    "    else:\n",
    "        if label in strings.columns:\n",
    "            labels = strings[label]\n",
    "        strings = strings[column]\n",
    "else:\n",
    "    strings = read_folder(in_file, in_type = in_type)\n",
    "    patterns = Series([\".*\" + x + \".*\" for x in open(patterns_file, 'r').readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending conversation together and creating spark data frome\n",
    "try:\n",
    "    strings['conversation'] = strings['conversation'].apply(lambda x: \". \".join(x[\"Message\"]))\n",
    "except:\n",
    "    pass\n",
    "sentenceDataFrame = spark.createDataFrame(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of sentences for each conversation\n",
    "sentenceDataFrame = tokenize_sentence_nltk_spark(df = sentenceDataFrame, in_col = \"conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language identification and filtering\n",
    "sentenceDataFrame = detect_language_spark(df = sentenceDataFrame, in_col = \"conversation\", out_col = \"language\")\n",
    "sentenceDataFrame = sentenceDataFrame.where(col('language') == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging and lemmatization using TreeTagger\n",
    "sentenceDataFrame = run_treetagger_pos_tag_spark(df = sentenceDataFrame, in_col = \"conversation\", out_col = \"pos\", get_lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 2 consecutive words if a) Words are incorrectly spelled and b) Merged word is correctly spelled\n",
    "sentenceDataFrame = spell_correct_tokens_spark(df = sentenceDataFrame, in_col = \"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening out token of rows and running word2vec model\n",
    "sentenceDataFrame = flatten_list_of_tokens(sentenceDataFrame, in_col = \"pos\")\n",
    "model, sentenceDataFrame = run_word2vec_model_pyspark(sentenceDataFrame, in_col = \"pos\", vec_size = 100, in_type = \"tokens\", out_col = \"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting document vectors in a list\n",
    "doc_vecs = []\n",
    "for row in sentenceDataFrame.select('result').collect():\n",
    "    doc_vecs = doc_vecs + [row['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1 = get_semantic_similarity_spark(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lover</th>\n",
       "      <th>rate</th>\n",
       "      <th>assert</th>\n",
       "      <th>termination</th>\n",
       "      <th>irs</th>\n",
       "      <th>california</th>\n",
       "      <th>e-meetings</th>\n",
       "      <th>scenario</th>\n",
       "      <th>nbsp</th>\n",
       "      <th>gardener</th>\n",
       "      <th>...</th>\n",
       "      <th>capacity</th>\n",
       "      <th>physical</th>\n",
       "      <th>brilliant</th>\n",
       "      <th>never</th>\n",
       "      <th>those</th>\n",
       "      <th>administrative</th>\n",
       "      <th>d</th>\n",
       "      <th>nickname</th>\n",
       "      <th>cash</th>\n",
       "      <th>only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lover</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029827</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>0.097563</td>\n",
       "      <td>0.116361</td>\n",
       "      <td>-0.334425</td>\n",
       "      <td>0.290318</td>\n",
       "      <td>-0.333660</td>\n",
       "      <td>-0.088317</td>\n",
       "      <td>-0.263155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366859</td>\n",
       "      <td>-0.018899</td>\n",
       "      <td>-0.409934</td>\n",
       "      <td>-0.338293</td>\n",
       "      <td>0.235658</td>\n",
       "      <td>0.303156</td>\n",
       "      <td>0.327642</td>\n",
       "      <td>-0.019418</td>\n",
       "      <td>-0.397134</td>\n",
       "      <td>-0.043701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate</th>\n",
       "      <td>-0.029827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577931</td>\n",
       "      <td>0.594535</td>\n",
       "      <td>-0.389364</td>\n",
       "      <td>0.634342</td>\n",
       "      <td>0.620144</td>\n",
       "      <td>0.358991</td>\n",
       "      <td>-0.205235</td>\n",
       "      <td>0.361067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485562</td>\n",
       "      <td>0.542137</td>\n",
       "      <td>0.237939</td>\n",
       "      <td>0.545577</td>\n",
       "      <td>0.113771</td>\n",
       "      <td>0.350214</td>\n",
       "      <td>0.219478</td>\n",
       "      <td>0.638139</td>\n",
       "      <td>0.298379</td>\n",
       "      <td>0.782942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assert</th>\n",
       "      <td>0.346149</td>\n",
       "      <td>0.577931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611674</td>\n",
       "      <td>-0.146607</td>\n",
       "      <td>0.318166</td>\n",
       "      <td>0.699524</td>\n",
       "      <td>0.192764</td>\n",
       "      <td>-0.448236</td>\n",
       "      <td>0.161652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239511</td>\n",
       "      <td>0.553953</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>0.258411</td>\n",
       "      <td>0.443083</td>\n",
       "      <td>0.518845</td>\n",
       "      <td>0.543767</td>\n",
       "      <td>0.468013</td>\n",
       "      <td>0.075075</td>\n",
       "      <td>0.615667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>termination</th>\n",
       "      <td>0.097563</td>\n",
       "      <td>0.594535</td>\n",
       "      <td>0.611674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.421173</td>\n",
       "      <td>0.318140</td>\n",
       "      <td>0.517693</td>\n",
       "      <td>0.148874</td>\n",
       "      <td>-0.601911</td>\n",
       "      <td>-0.038200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289914</td>\n",
       "      <td>0.383086</td>\n",
       "      <td>-0.010234</td>\n",
       "      <td>0.320089</td>\n",
       "      <td>0.070759</td>\n",
       "      <td>0.150894</td>\n",
       "      <td>0.198482</td>\n",
       "      <td>0.574032</td>\n",
       "      <td>0.178088</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irs</th>\n",
       "      <td>0.116361</td>\n",
       "      <td>-0.389364</td>\n",
       "      <td>-0.146607</td>\n",
       "      <td>-0.421173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.395529</td>\n",
       "      <td>-0.234021</td>\n",
       "      <td>0.038187</td>\n",
       "      <td>-0.222186</td>\n",
       "      <td>0.151884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134450</td>\n",
       "      <td>-0.177418</td>\n",
       "      <td>0.041391</td>\n",
       "      <td>-0.249011</td>\n",
       "      <td>0.587008</td>\n",
       "      <td>0.260640</td>\n",
       "      <td>0.560561</td>\n",
       "      <td>-0.522951</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.611751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                lover      rate    assert  termination       irs  california  \\\n",
       "lover        1.000000 -0.029827  0.346149     0.097563  0.116361   -0.334425   \n",
       "rate        -0.029827  1.000000  0.577931     0.594535 -0.389364    0.634342   \n",
       "assert       0.346149  0.577931  1.000000     0.611674 -0.146607    0.318166   \n",
       "termination  0.097563  0.594535  0.611674     1.000000 -0.421173    0.318140   \n",
       "irs          0.116361 -0.389364 -0.146607    -0.421173  1.000000   -0.395529   \n",
       "\n",
       "             e-meetings  scenario      nbsp  gardener    ...     capacity  \\\n",
       "lover          0.290318 -0.333660 -0.088317 -0.263155    ...    -0.366859   \n",
       "rate           0.620144  0.358991 -0.205235  0.361067    ...     0.485562   \n",
       "assert         0.699524  0.192764 -0.448236  0.161652    ...     0.239511   \n",
       "termination    0.517693  0.148874 -0.601911 -0.038200    ...     0.289914   \n",
       "irs           -0.234021  0.038187 -0.222186  0.151884    ...    -0.134450   \n",
       "\n",
       "             physical  brilliant     never     those  administrative  \\\n",
       "lover       -0.018899  -0.409934 -0.338293  0.235658        0.303156   \n",
       "rate         0.542137   0.237939  0.545577  0.113771        0.350214   \n",
       "assert       0.553953  -0.026611  0.258411  0.443083        0.518845   \n",
       "termination  0.383086  -0.010234  0.320089  0.070759        0.150894   \n",
       "irs         -0.177418   0.041391 -0.249011  0.587008        0.260640   \n",
       "\n",
       "                    d  nickname      cash      only  \n",
       "lover        0.327642 -0.019418 -0.397134 -0.043701  \n",
       "rate         0.219478  0.638139  0.298379  0.782942  \n",
       "assert       0.543767  0.468013  0.075075  0.615667  \n",
       "termination  0.198482  0.574032  0.178088  0.671300  \n",
       "irs          0.560561 -0.522951 -0.229981 -0.611751  \n",
       "\n",
       "[5 rows x 2568 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = get_character_similarity(sim1.columns, ratio_type = 'ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_ratio = get_character_similarity(sim1.columns, ratio_type = 'partial_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sort_ratio = get_character_similarity(sim1.columns, ratio_type = 'token_sort_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set_ratio = get_character_similarity(sim1.columns, ratio_type = 'token_set_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1 = sim1[ratio.columns]\n",
    "sim1 = sim1.loc[ratio.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_weight = 0.5\n",
    "ratio_weight = 0.4\n",
    "partial_ratio_weight = 0.4\n",
    "token_sort_ratio_weight = 0.1\n",
    "sim = 1 - (semantic_weight*sim1 + (ratio_weight*ratio + partial_ratio_weight*partial_ratio + token_sort_ratio_weight*token_sort_ratio + (1-ratio_weight-partial_ratio_weight-token_sort_ratio_weight)*token_set_ratio)*(1-semantic_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
